{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation script for generating translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import *\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('./Training Dataset-20190429/train_data.csv')\n",
    "trainLabel = pd.read_csv('./Training Dataset-20190429/train_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating other language into English version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path defined\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/dt/Documents/key.json\"\n",
    "\n",
    "from google.cloud import translate\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "# translate the list of text into english text\n",
    "def translate_text(text, target = 'en'):\n",
    "    transalte_client = translate.Client()\n",
    "    translation = transalte_client.translate(\n",
    "        text,\n",
    "        target_language=target\n",
    "    )\n",
    "    print('Text: ', translation['input'] )\n",
    "    print('Translation: ', translation['translatedText'])\n",
    "    print('Detected source: ', translation['detectedSourceLanguage'])\n",
    "    return translation['translatedText']\n",
    "\n",
    "\n",
    "# check the text language type\n",
    "def get_language_id(dataDf):\n",
    "    lang = []\n",
    "    for index, row in dataDf.iterrows():\n",
    "        try:\n",
    "            lang.append(detect(row[\"text\"]))\n",
    "        except:\n",
    "            lang.append('error')\n",
    "            \n",
    "    trn = dataDf[\"trn_id\"].values\n",
    "    text = dataDf[\"text\"].values\n",
    "    df = pd.DataFrame(np.array([trn, text, lang])).T\n",
    "    df.columns = [\"trn_id\", \"text\", \"lang\"]\n",
    "    return df\n",
    "\n",
    "# translate all non english type texts into dictionary with trn ID\n",
    "def translate_non_eng_dic(dataLangDf):\n",
    "    dataLangTransDf = dataLangDf.copy()\n",
    "    langList = list(set(dataLangDf[\"lang\"].values))\n",
    "    listOfKeys = [index  for (index, row) in dataLangDf.iterrows() if row[\"lang\"] != \"en\"]\n",
    "    for index in listOfKeys:\n",
    "        dataLangTransDf.loc[index, \"text\"] = translate_text(dataLangDf.loc[index, \"text\"]) \n",
    "    dataLangTransDf[\"org_text\"] = dataLangDf[\"text\"]\n",
    "    dataLangTransDf.rename(columns = {\"text\": \"trans_text\"}, inplace=True)\n",
    "    return dataLangTransDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csvfiles_for_translated_data(data):\n",
    "\n",
    "    totalRecLen = len(data)\n",
    "    recLen = 100000\n",
    "    segRecLenL = 0\n",
    "    segRecLenR = recLen\n",
    "    \n",
    "    while (segRecLenR <= totalRecLen):\n",
    "        segData = data[segRecLenL:segRecLenR]\n",
    "        # detect how many non-English text\n",
    "        segDataLang = get_language_id(segData)\n",
    "        # translate other language into English and records those trnIDs\n",
    "        segDataLangTrans = translate_non_eng_dic(segDataLang)\n",
    "        fileName = \"trainDaTrans\" + str(segRecLenL) + \"to\"  + str(segRecLenR) +  \".csv\"\n",
    "        # write dataframe into csv file \n",
    "        segDataLangTrans.to_csv(fileName)\n",
    "        segRecLenL = segRecLenR\n",
    "        segRecLenR = segRecLenR + recLen\n",
    "        \n",
    "        if (segRecLenR > totalRecLen and segRecLenL < totalRecLen):\n",
    "            segRecLenR = totalRecLen\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_transfiles_dataframe(segRecLenL = 0, segRecLenR = 100000):\n",
    "    segRecLen = segRecLenR - segRecLenL\n",
    "    sum_df = pd.DataFrame()\n",
    "    \n",
    "    while (segRecLenR <= 650000):\n",
    "        fileName = \"trainDaTrans\" + str(segRecLenL) + \"to\"  + str(segRecLenR) +  \".csv\"\n",
    "\n",
    "        each_df = pd.read_csv(fileName, index_col=[0])\n",
    "        sum_df = pd.concat([sum_df,each_df],ignore_index=True)\n",
    "        \n",
    "        segRecLenL = segRecLenR\n",
    "        segRecLenR = segRecLenR + segRecLen\n",
    "        \n",
    "        if (segRecLenL == 600000):\n",
    "            segRecLenR = 650000\n",
    "    return sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dic_into_json(documentDic, fileName):\n",
    "    fileName = fileName + \".json\"\n",
    "    with open(fileName, 'w') as outfile:\n",
    "        json.dump(documentDic, outfile)\n",
    "    outfile.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating the data into english one\n",
    "# write_csvfiles_for_translated_data(trainData)\n",
    "\n",
    "# read the translated data from file\n",
    "trainDataTrans = union_transfiles_dataframe()\n",
    "\n",
    "# detect non English text and record it's trnID\n",
    "# reshape the dataframe with two columns trn_id and trans_text left\n",
    "trainDataTransNoteng = trainDataTrans[trainDataTrans[\"lang\"] != 'en']\n",
    "trainDataTransNoteng = trainDataTransNoteng[[\"trn_id\",\"trans_text\"]].rename(columns={\"trn_id\":\"trn_id\",\"trans_text\":\"text\"})\n",
    "\n",
    "# detect the new version text for recording text language type\n",
    "# find out all non English text trn_id documents\n",
    "langDf = get_language_id(trainDataTransNoteng)\n",
    "dropList = list(langDf[langDf[\"lang\"] != 'en'][\"trn_id\"].values)\n",
    "\n",
    "# reshape the dataframe with two columns trn_id and trans_text left\n",
    "# remove all other language documents\n",
    "trainDataTransReshape = trainDataTrans[[\"trn_id\",\"trans_text\"]].rename(columns={\"trn_id\":\"trn_id\",\"trans_text\":\"text\"})\n",
    "trainDataTransReshape = trainDataTransReshape[~trainDataTransReshape[\"trn_id\"].isin(dropList)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe into dictionary\n",
    "trDataDic = trainDataTransReshape.set_index('trn_id').T.to_dict('list')\n",
    "trDataDic = dict((trnID, trDataDic[trnID][0]) for trnID in trDataDic.keys())\n",
    "\n",
    "# write the dictionary into json file\n",
    "# write_dic_into_json(trDataDic, \"trDataDic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
