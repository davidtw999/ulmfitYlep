{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright (c) by Wei Tan Date:** 19 Aug 2019\n",
    "\n",
    "**Environment Factors:**\n",
    "* python 3.6\n",
    "* fastai 1.0\n",
    "* Cuda 10.0 GPU\n",
    "* pytorch 1.1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sentiment analysis or commonly known as opinion mining is the process of using machine learning to analyse a person’s opinion, emotional tone and attitude based on a written text (Liu, 2012). Research in the field of sentiment analysis such as identifying metaphor and expression started in the early 90s with the term sentiment analysis was first introduced in 2003 (Liu, 2012). \n",
    "  \n",
    "The Application of sentiment analysis includes social media monitoring of events. For example, Obama administration used sentiment analysis on social media platform such as Facebook and Twitter to measure the public’s opinion ahead of his presidential election which he subsequently won or big brands company such as Coca Cola using sentiment analysis to gauge the public reaction of their product or the public perception of the company. By using sentiment analysis, companies are able to improve customer service, product quality or even develop and adjust their marketing strategy. \n",
    "  \n",
    "The code structure is shown as below:\n",
    "* Loading the dataset\n",
    "* Preprocessing the dataset\n",
    "* Fine-tuning a language model\n",
    "* Develop the classifier model\n",
    "* Retuning the saved model\n",
    "* Predict the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import html\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fastai import * \n",
    "from fastai.text import * \n",
    "from fastai.core import *\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a path for saving dataset and model\n",
    "DATA_PATH=Path('saved')\n",
    "SAVE_PATH=Path('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training and testing dataset\n",
    "df_tr = pd.read_csv(DATA_PATH/'trainData.csv')\n",
    "df_te = pd.read_csv(DATA_PATH/'testDataTrans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the column name of dataframe into the specific format\n",
    "# df_trLM and df_te is for language model use\n",
    "# df_trCL is for classifier model use\n",
    "df_trLM = pd.DataFrame({0:df_tr[\"text\"],1:df_tr[\"text\"]})\n",
    "df_trCL = pd.DataFrame({0:df_tr[\"label\"],1:df_tr[\"text\"]})\n",
    "df_te = pd.DataFrame({0:df_te[\"text\"],1:df_te[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union df_trLM and df_trCL as one training dataset for language model use\n",
    "dfoutAll = pd.concat([df_trLM,df_te],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable and save it into a file for later language model use\n",
    "trainDataTL = dfoutAll[[0,1]]\n",
    "trainDataTL.to_csv(SAVE_PATH/\"trainDataTL.csv\", header=None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataBunch object that is used inside Learner to train a model\n",
    "# this does all the necessary preprocessing behind the scene\n",
    "# It basically creates a separate unit (a “token”) for each separate part of a word. \n",
    "# Most of them are just for words, but sometimes if it’s an 's from it's, \n",
    "# it will get its own token. Every bit of punctuation tends to \n",
    "# get its own token (a comma, a period, etc).\n",
    "data_lm = TextLMDataBunch.from_csv(SAVE_PATH, 'trainDataTL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the previous DataBunch data as pickle format for training language model use\n",
    "data_lm.save('data_lm_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved the pickle data for training language model use\n",
    "data_lm = load_data(SAVE_PATH,'data_lm_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>another employee in the middle of our order . \\r \\n \\r \\n  i had high hopes for xxmaj pollo xxmaj campero , as every time i drove past it , i could see cars wrapping around the drive through . i realize now it is because they have inefficient work . xxmaj it took about 15 - 20 minutes to get it our food , which is surprising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>... xxmaj vegas is definitely still on my mind ... \\r \\n  xxmaj we had the fantastic xxmaj xxunk xxmaj suite on the 27th xxmaj floor and it was spic xxrep 4 y . xxmaj enough to easily sleep 6 + people , although we were only 3 deep . \\r \\n  xxmaj shower , separate bath tub and private toilet area . \\r \\n  xxmaj large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>xxmaj allegheny xxmaj tavern last night . xxmaj we had been looking forward to a night out together for a while . \\r \\n \\r \\n  i think the food was good , but it 's hard to say , because all i could focus on was the xxup loud -- and i mean xxup loud -- woman seated behind my boyfriend and me . \\r \\n \\r \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ever in my life xxrep 4 . xxbos xxmaj they worse dunking donuts ever xxrep 5 ! \\r \\n  xxmaj first they do n't know how to make coffee ! ! ! \\r \\n  xxmaj they have so many employees and it 's look like they do n't want to give service . \\r \\n  xxmaj the place very messy and dirty . xxbos xxmaj as you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>do not see it lasting much longer . xxmaj there are other places in xxmaj calgary to get sub - par broth and far bigger portion of meat and veggies . i think i will stick to xxunk for now . xxbos xxmaj it was a very disappointing dining experience . xxmaj there was a long line for this restaurant ( and decent reviews on xxmaj yelp ) , which</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the tokenization text\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first ten in order of frequency\n",
    "# output list is all the possible unique tokens\n",
    "data_lm.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.119113</td>\n",
       "      <td>3.982092</td>\n",
       "      <td>0.272577</td>\n",
       "      <td>50:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.135674</td>\n",
       "      <td>4.005588</td>\n",
       "      <td>0.270786</td>\n",
       "      <td>50:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.100678</td>\n",
       "      <td>3.962234</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>50:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.047487</td>\n",
       "      <td>3.912771</td>\n",
       "      <td>0.278795</td>\n",
       "      <td>50:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.027775</td>\n",
       "      <td>3.896547</td>\n",
       "      <td>0.280431</td>\n",
       "      <td>50:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a model as learn object, and download the pretrained wiki103 model and be ready for fine-tuning\n",
    "# set AWD-LSTM method to tune\n",
    "# drop_mult=0.3 is for reducing the regularization to avoid under fitting\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, pretrained_fnames = [\"lstm_wt103\",\"itos_wt103\"],drop_mult=0.3)\n",
    "# create a output word embedding vector as the input encoder by training the language model \n",
    "# tuning from wiki103 model for classifier model use \n",
    "# set 5 epoches and learning rate 0.01\n",
    "learn.fit_one_cycle(5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.552196</td>\n",
       "      <td>3.460151</td>\n",
       "      <td>0.331855</td>\n",
       "      <td>55:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.449042</td>\n",
       "      <td>3.379631</td>\n",
       "      <td>0.341974</td>\n",
       "      <td>55:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unfreeze all the layers of the model and fine-tune it\n",
    "learn.unfreeze()\n",
    "# set 2 epoches, learing rate 0.01 and momentum equals 0.8,0.7. \n",
    "# Basically fastai found for training RNNs, it really helps to decrease the momentum a little bit\n",
    "learn.fit_one_cycle(2, 1e-3,moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our language model by predict the certain words by passing serval words \n",
    "learn.predict(\"This is a review about\", n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the encoder for classifier model use\n",
    "learn.save_encoder('ft_enc_wk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the classifier model\n",
    "* unfreeze the last two layers\n",
    "* train it a little bit more\n",
    "* unfreeze the next layer again\n",
    "* train it a little bit more\n",
    "* unfreeze the whole thing\n",
    "* train it a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training dataset for building the classifier model\n",
    "df_trCL.to_csv(SAVE_PATH/\"trainDataCLS.csv\", header=None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the previous generated dataset for building \n",
    "# pass the vocabulary (mapping from ids to words) created from language model that we want to use\n",
    "# set validation ratio as 10% and batch size 32, make lower if you run out of memory\n",
    "data_clas = TextClasDataBunch.from_csv(SAVE_PATH, 'trainDataCLS.csv',valid_pct=0.1, vocab=data_lm.train_ds.vocab, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the previous DataBunch data as pickle format for training classifier model use\n",
    "data_clas.save('data_clas_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved the pickle data for training classifier model use\n",
    "data_clas = load_data(SAVE_PATH,'data_clas_export.pkl', bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a learner object by using the data_clas object to build a classifier \n",
    "# load the fine-tuned the encoder input created from language model\n",
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder('ft_enc_wk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj we xxmaj have xxmaj been xxmaj regulars at xxmaj several xxmaj outbacks xxmaj usually xxmaj by xxmaj where xxmaj we xxmaj live . xxmaj for xxmaj years xxmaj went xxup xxunk xxmaj scottsdale xxmaj location xxmaj because it xxmaj was xxmaj near xxmaj the xxmaj harkins xxmaj there . xxmaj never xxmaj had a xxmaj problem . xxmaj then xxmaj we xxmaj moved &amp; xxmaj we xxmaj were</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxup beware ! \\r \\n \\r \\n  i believe it was back in 2007 when my best friend and i decided to get gym memberships at this 24 xxmaj hour xxmaj fitness . xxmaj she signed up for the month - to - month option where they debit your monthly fee from your account . i was not comfortable with this and after severe pressure from one of</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj in an age where teenagers are getting $ xxunk an hour to babysit children ( yes , that 's live xxup human beings ) , i find it astonishing that an xxup az pet sitting service would have the audacity to charge $ 32 for a 1 / 2 hour cat visit ! xxmaj if you do the math , that 's $ 64 / hour folks ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i want to start off by saying , this was the most horrible experience ever we had renting with this particular xxmaj alamo located at the mccarran xxmaj alamo xxmaj rent a xxmaj car xxmaj center . xxmaj my xxmaj father who rented it through xxunk . xxmaj com was visiting from xxmaj hawaii . \\r \\n \\r \\n  xxmaj we get to the counter and i really</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos i am giving this xxup scam absolutely no stars ! ! ! \\r \\n \\r \\n  xxmaj the marketing team for this so called resort preys upon unsuspecting tourists as they innocently travel through xxmaj sin xxmaj city . xxmaj this group woos you with the promise of a free gift ( dinner show even a 2 day cruise ! ) xxmaj and all you have to do</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the data_clas object content which includes \"text\" and \"label\"\n",
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.917727</td>\n",
       "      <td>0.835745</td>\n",
       "      <td>0.635626</td>\n",
       "      <td>18:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.922176</td>\n",
       "      <td>0.860451</td>\n",
       "      <td>0.634272</td>\n",
       "      <td>19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.888794</td>\n",
       "      <td>0.814937</td>\n",
       "      <td>0.643722</td>\n",
       "      <td>16:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.865585</td>\n",
       "      <td>0.804189</td>\n",
       "      <td>0.650310</td>\n",
       "      <td>17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.650125</td>\n",
       "      <td>16:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run 5 epoches with learning rate 0.01 to train the model\n",
    "learn.fit_one_cycle(5, 1e-2, moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after 5 epoch\n",
    "learn.save('first')\n",
    "learn.load('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.746111</td>\n",
       "      <td>0.703349</td>\n",
       "      <td>0.700134</td>\n",
       "      <td>20:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unfreeze last two layers and other layers keep freeze and fune-tune last 2 layers\n",
    "# set 1 epoch and learning rate 1e-2/(2.6**4) is better for discriminating learning\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2), moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after tuning the last 2 layers\n",
    "learn.save('second')\n",
    "learn.load('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.696538</td>\n",
       "      <td>0.669784</td>\n",
       "      <td>0.710801</td>\n",
       "      <td>39:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then unfreeze the last third layer and tuning it more\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3), moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after tuning the layers\n",
    "learn.save('third')\n",
    "learn.load('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.669128</td>\n",
       "      <td>0.663296</td>\n",
       "      <td>0.713787</td>\n",
       "      <td>54:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640030</td>\n",
       "      <td>0.659572</td>\n",
       "      <td>0.716142</td>\n",
       "      <td>50:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unfreeze all layers and tuning it\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3), moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weight\n",
    "learn.save(\"yelpModel716142\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.629368</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.717050</td>\n",
       "      <td>48:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-3/(2.6**4), 1e-3), moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model after tuning the layers\n",
    "learn.save('yelpModel717050')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retuning the saved model if need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (584712 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this is probably the worst business i have ever dealt with . xxmaj none of their people know xxmaj english and they rude as hell . i tried to downgrade to just wifi and they would n't let me do it . i provided my last four of my social and card number and they still would n't let me in my account because i did n't know their fake ass pin they claim that i made . xxmaj this is a tactic they are using just to keep me at the package i 'm at . xxmaj cox stinks,xxbos xxmaj selection walking in was xxunk . xxmaj the cookies are okay but nothing to return back for .,xxbos xxmaj beyond my expectations ! xxmaj stunning combinations of flowers , colors . xxmaj and the fragrance ! xxmaj so happy to make my daughter happy on her wedding day .,xxbos i stand corrected . xxmaj there is a xxmaj top xxmaj shop inside xxmaj nordstrom in xxmaj union xxmaj square . xxmaj dangerously awesome ... \n",
       " \n",
       "  xxrep 26 _ \n",
       " \n",
       "  xxmaj topshop , xxmaj topshop , xxmaj topshop ! ! ! \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj can you tell how excited i was to hit up xxmaj topshop inside xxmaj hudson 's xxmaj bay ? ! ? xxmaj why we do n't yet have one in xxmaj san xxmaj francisco is baffling . xxmaj though it 's a double - edged sword ; as i fear if we did , many of us would go broke ! \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj shopping in another country is always fun since the prices often mean nothing . xxmaj buy what you like and deal with the consequences when you get home . xxmaj it 's not like i went wild and purchased a whole new wardrobe , just a few cutesy spring pieces and new sunglasses that i hope will last longer than my last xxmaj american xxmaj apparel debacle . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj hudson 's xxmaj bay is fancy ! xxmaj way better than any xxup us department store . xxmaj reminded me of xxmaj xxunk 's , minus the xxunk hairdos and air of pretense . i especially liked the woman behind one of the cosmetics counters who provided directions to xxmaj top xxmaj shop , she was especially chic with her xxmaj french \" xxmaj xxunk \" accent . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj consider this review an official petition to bring xxmaj top xxmaj shop to xxup sf ...,xxbos xxmaj nice place , but the hostess girls are extremely rude ! xxmaj staff is caught up in only serving people that look like they have money . i would say pass it up and head to city hall or steak 44 .\n",
       "y: CategoryList\n",
       "1,3,5,4,2\n",
       "Path: saved;\n",
       "\n",
       "Valid: LabelList (64969 items)\n",
       "x: TextList\n",
       "xxbos xxmaj came here for brunch during the holidays . xxmaj the place is xxup tiny , we had to wait about 30 mins to get seated , which was okay and expected . xxmaj with that said i would n't come here if i had a group of more than 4 people . \n",
       " \n",
       " \n",
       " \n",
       "  i got the standard breakfast ( forgot what it was called ) that came with whole grain toast , bacon , home fries and 2 eggs made to order ( i got them poached ) and a mimosa . xxmaj my boyfriend got the eggs benny and a caesar . xxmaj my xxunk was delicious , the home fries were made xxunk and the bacon was cooked just the way i like it ( crispy ) . xxmaj my poached egg was a little too well done but what can you do ... xxmaj my boyfriend really enjoyed his eggs benny and his eggs were perfectly made - not too well done like mine . xxmaj the portions were a good amount and we were stuffed once finished . xxmaj my boyfriend said the caesar had an odd taste to it initially but then once he was finished he said it really enjoyed it ! ( boys .. ca n't make up their minds lol ) xxmaj our whole meal came to about $ 40 after tip , which was good considering i 've paid much more for pretty much the same thing at other places . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj service was nice , but not as attentive as i would have liked - maybe because we were sitting near the back . i 'll definitely be coming back here to try their famous fried chicken during dinner time !,xxbos xxmaj pretty xxmaj great xxmaj greek food ... xxmaj the salads are very tasty and of course the gyros are yummy . xxmaj we used to love it here when they danced and threw the plates but these days it is more sedate . xxmaj if you are playing video poker , they do not give you any drink free - only bar brands ... xxmaj you can sit and wait for carry out but beware the video poker machines were not very friendly or loose when i was there : (,xxbos xxmaj mitchell 's was surprisingly good . xxmaj everything i tasted was good or better , and there were plenty of options . xxmaj everything i drank was good or better , and there are options for everyone . xxmaj the atmosphere was great ( especially sitting outside on one of the rare nice days ) , which definitely enhanced the experience ( though do n't count on that part being reliable in xxmaj pittsburgh ... ) . xxmaj my dings against the place are mostly minor ( e.g. , tiny hiccups in service ) , but for one : it 's a bit pricey for the category in which i 'd place it , though it 's still a great option in that category .,xxbos xxmaj there crepes are amazing ! ! i got great xxmaj belgium for the first time and xxmaj alaskan with eggs for the second time . xxmaj love walnut street only because of all the great restaurants you can check out here !,xxbos xxmaj good cooking techniques , creative presentation , but boring sauces that lack variety with fancy names , smokey flavour in the shrimp cakes will remind you of ball park hot dogs . xxmaj exception was the shrimp tamale - quite tasty . xxup ps xxmaj bobby nothing personal but dancing on the cutting board did n't add any flavours to the food .\n",
       "y: CategoryList\n",
       "4,3,4,5,2\n",
       "Path: saved;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60004, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60004, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04000000000000001)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f0d23b10a60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('saved'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (584712 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this is probably the worst business i have ever dealt with . xxmaj none of their people know xxmaj english and they rude as hell . i tried to downgrade to just wifi and they would n't let me do it . i provided my last four of my social and card number and they still would n't let me in my account because i did n't know their fake ass pin they claim that i made . xxmaj this is a tactic they are using just to keep me at the package i 'm at . xxmaj cox stinks,xxbos xxmaj selection walking in was xxunk . xxmaj the cookies are okay but nothing to return back for .,xxbos xxmaj beyond my expectations ! xxmaj stunning combinations of flowers , colors . xxmaj and the fragrance ! xxmaj so happy to make my daughter happy on her wedding day .,xxbos i stand corrected . xxmaj there is a xxmaj top xxmaj shop inside xxmaj nordstrom in xxmaj union xxmaj square . xxmaj dangerously awesome ... \n",
       " \n",
       "  xxrep 26 _ \n",
       " \n",
       "  xxmaj topshop , xxmaj topshop , xxmaj topshop ! ! ! \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj can you tell how excited i was to hit up xxmaj topshop inside xxmaj hudson 's xxmaj bay ? ! ? xxmaj why we do n't yet have one in xxmaj san xxmaj francisco is baffling . xxmaj though it 's a double - edged sword ; as i fear if we did , many of us would go broke ! \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj shopping in another country is always fun since the prices often mean nothing . xxmaj buy what you like and deal with the consequences when you get home . xxmaj it 's not like i went wild and purchased a whole new wardrobe , just a few cutesy spring pieces and new sunglasses that i hope will last longer than my last xxmaj american xxmaj apparel debacle . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj hudson 's xxmaj bay is fancy ! xxmaj way better than any xxup us department store . xxmaj reminded me of xxmaj xxunk 's , minus the xxunk hairdos and air of pretense . i especially liked the woman behind one of the cosmetics counters who provided directions to xxmaj top xxmaj shop , she was especially chic with her xxmaj french \" xxmaj xxunk \" accent . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj consider this review an official petition to bring xxmaj top xxmaj shop to xxup sf ...,xxbos xxmaj nice place , but the hostess girls are extremely rude ! xxmaj staff is caught up in only serving people that look like they have money . i would say pass it up and head to city hall or steak 44 .\n",
       "y: CategoryList\n",
       "1,3,5,4,2\n",
       "Path: saved;\n",
       "\n",
       "Valid: LabelList (64969 items)\n",
       "x: TextList\n",
       "xxbos xxmaj came here for brunch during the holidays . xxmaj the place is xxup tiny , we had to wait about 30 mins to get seated , which was okay and expected . xxmaj with that said i would n't come here if i had a group of more than 4 people . \n",
       " \n",
       " \n",
       " \n",
       "  i got the standard breakfast ( forgot what it was called ) that came with whole grain toast , bacon , home fries and 2 eggs made to order ( i got them poached ) and a mimosa . xxmaj my boyfriend got the eggs benny and a caesar . xxmaj my xxunk was delicious , the home fries were made xxunk and the bacon was cooked just the way i like it ( crispy ) . xxmaj my poached egg was a little too well done but what can you do ... xxmaj my boyfriend really enjoyed his eggs benny and his eggs were perfectly made - not too well done like mine . xxmaj the portions were a good amount and we were stuffed once finished . xxmaj my boyfriend said the caesar had an odd taste to it initially but then once he was finished he said it really enjoyed it ! ( boys .. ca n't make up their minds lol ) xxmaj our whole meal came to about $ 40 after tip , which was good considering i 've paid much more for pretty much the same thing at other places . \n",
       " \n",
       " \n",
       " \n",
       "  xxmaj service was nice , but not as attentive as i would have liked - maybe because we were sitting near the back . i 'll definitely be coming back here to try their famous fried chicken during dinner time !,xxbos xxmaj pretty xxmaj great xxmaj greek food ... xxmaj the salads are very tasty and of course the gyros are yummy . xxmaj we used to love it here when they danced and threw the plates but these days it is more sedate . xxmaj if you are playing video poker , they do not give you any drink free - only bar brands ... xxmaj you can sit and wait for carry out but beware the video poker machines were not very friendly or loose when i was there : (,xxbos xxmaj mitchell 's was surprisingly good . xxmaj everything i tasted was good or better , and there were plenty of options . xxmaj everything i drank was good or better , and there are options for everyone . xxmaj the atmosphere was great ( especially sitting outside on one of the rare nice days ) , which definitely enhanced the experience ( though do n't count on that part being reliable in xxmaj pittsburgh ... ) . xxmaj my dings against the place are mostly minor ( e.g. , tiny hiccups in service ) , but for one : it 's a bit pricey for the category in which i 'd place it , though it 's still a great option in that category .,xxbos xxmaj there crepes are amazing ! ! i got great xxmaj belgium for the first time and xxmaj alaskan with eggs for the second time . xxmaj love walnut street only because of all the great restaurants you can check out here !,xxbos xxmaj good cooking techniques , creative presentation , but boring sauces that lack variety with fancy names , smokey flavour in the shrimp cakes will remind you of ball park hot dogs . xxmaj exception was the shrimp tamale - quite tasty . xxup ps xxmaj bobby nothing personal but dancing on the cutting board did n't add any flavours to the food .\n",
       "y: CategoryList\n",
       "4,3,4,5,2\n",
       "Path: saved;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60004, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60004, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04000000000000001)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f0d23b10a60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('saved'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60004, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60004, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04000000000000001)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=None)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60004, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60004, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 1150, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1150, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.04000000000000001)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the saved model for predition\n",
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.1)\n",
    "learn.load_encoder('ft_enc_wk')\n",
    "learn.load(\"2222222\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the momentum\n",
    "learn.lr_find()\n",
    "learn.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "learn.lr_find()\n",
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the model and fune-tune last 2 layers\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3), moms = (0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warning message\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define a prectiction function\n",
    "def pred_labels(learnFit):\n",
    "    testDataDf = pd.read_csv(SAVE_PATH/\"testDataTrans.csv\")\n",
    "    teDataDfArray = testDataDf[\"text\"].values\n",
    "    labels = []\n",
    "    for x in tqdm(teDataDfArray):\n",
    "        pred = learnFit.predict(x)\n",
    "        labels.append(pred[0])\n",
    "    predLabels = [int(str(x)) for x in labels]\n",
    "    predLabelsDF = pd.DataFrame({\"test_id\":testDataDf[\"test_id\"].values, \"label\":predLabels})\n",
    "    predLabelsDF.to_csv(\"predLabels.csv\", index=False)\n",
    "    return predLabelsDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [1:15:40<00:00, 12.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test_14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test_20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test_21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>test_22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test_23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>test_24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>test_25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test_26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>test_27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>test_28</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>test_29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>test_30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>test_49971</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>test_49972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>test_49973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>test_49974</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>test_49975</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>test_49976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>test_49977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>test_49978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>test_49979</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>test_49980</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>test_49981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>test_49982</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>test_49983</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>test_49984</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>test_49985</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>test_49986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>test_49987</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>test_49988</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>test_49989</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>test_49990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>test_49991</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>test_49992</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>test_49993</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>test_49994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>test_49995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>test_49996</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>test_49997</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>test_49998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>test_49999</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>test_50000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          test_id  label\n",
       "0          test_1      2\n",
       "1          test_2      4\n",
       "2          test_3      1\n",
       "3          test_4      5\n",
       "4          test_5      4\n",
       "5          test_6      4\n",
       "6          test_7      3\n",
       "7          test_8      4\n",
       "8          test_9      2\n",
       "9         test_10      1\n",
       "10        test_11      1\n",
       "11        test_12      3\n",
       "12        test_13      4\n",
       "13        test_14      2\n",
       "14        test_15      5\n",
       "15        test_16      5\n",
       "16        test_17      1\n",
       "17        test_18      4\n",
       "18        test_19      5\n",
       "19        test_20      1\n",
       "20        test_21      5\n",
       "21        test_22      4\n",
       "22        test_23      5\n",
       "23        test_24      2\n",
       "24        test_25      3\n",
       "25        test_26      1\n",
       "26        test_27      5\n",
       "27        test_28      5\n",
       "28        test_29      5\n",
       "29        test_30      1\n",
       "...           ...    ...\n",
       "49970  test_49971      3\n",
       "49971  test_49972      1\n",
       "49972  test_49973      1\n",
       "49973  test_49974      5\n",
       "49974  test_49975      3\n",
       "49975  test_49976      1\n",
       "49976  test_49977      1\n",
       "49977  test_49978      1\n",
       "49978  test_49979      2\n",
       "49979  test_49980      4\n",
       "49980  test_49981      3\n",
       "49981  test_49982      5\n",
       "49982  test_49983      4\n",
       "49983  test_49984      4\n",
       "49984  test_49985      2\n",
       "49985  test_49986      2\n",
       "49986  test_49987      2\n",
       "49987  test_49988      4\n",
       "49988  test_49989      4\n",
       "49989  test_49990      1\n",
       "49990  test_49991      2\n",
       "49991  test_49992      2\n",
       "49992  test_49993      5\n",
       "49993  test_49994      3\n",
       "49994  test_49995      5\n",
       "49995  test_49996      3\n",
       "49996  test_49997      2\n",
       "49997  test_49998      2\n",
       "49998  test_49999      5\n",
       "49999  test_50000      2\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction labels by using the past model\n",
    "pred_labels(learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
